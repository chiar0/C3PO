quarkus.http.port=9800

# Log dettagliati per individuare problemi
quarkus.log.level=INFO
quarkus.log.category."com.ai".level=DEBUG
quarkus.log.category."dev.langchain4j".level=DEBUG
quarkus.log.category."io.quarkiverse.langchain4j".level=DEBUG

# Configurazione per Ollama in locale
quarkus.langchain4j.ollama.chat-model.base-url=http://localhost:11434
quarkus.langchain4j.ollama.chat-model.model-name=qwen3:0.6b-fp16

# Timeout specifico per Ollama (funziona solo con versione recente)
quarkus.langchain4j.ollama.chat-model.timeout=120
quarkus.langchain4j.ollama.timeout=60s

# Timeout globale per REST Client (necessario)
quarkus.rest-client.connect-timeout=120
quarkus.rest-client.read-timeout=120

# Timeout Vert.x (se usi Quarkus)
quarkus.vertx.event-loops-pool-size=10

# Disabilita le propriet√† di OpenAI (non necessarie per Ollama)
quarkus.langchain4j.openai.api-key=
quarkus.langchain4j.openai.log-requests=false
quarkus.langchain4j.openai.log-responses=false
quarkus.rest-client.openrouter-api.read-timeout=120000

# Attiva logging delle richieste/risposte API
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true